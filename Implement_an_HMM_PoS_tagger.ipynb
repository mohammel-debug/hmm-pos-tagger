{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/mohammel-debug/hmm-pos-tagger/blob/main/Implement_an_HMM_PoS_tagger.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# dataset 1: CHILDES\n",
        "!wget -q https://raw.githubusercontent.com/UniversalDependencies/UD_English-CHILDES/master/en_childes-ud-train.conllu\n",
        "!wget -q https://raw.githubusercontent.com/UniversalDependencies/UD_English-CHILDES/master/en_childes-ud-dev.conllu\n",
        "!wget -q https://raw.githubusercontent.com/UniversalDependencies/UD_English-CHILDES/master/en_childes-ud-test.conllu\n",
        "\n",
        "\n",
        "# dataset 2: Web Text (EWT)\n",
        "# Download EWT dataset with exact filenames\n",
        "!wget -O /content/en_ewt-ud-train.conllu https://raw.githubusercontent.com/UniversalDependencies/UD_English-EWT/master/en_ewt-ud-train.conllu\n",
        "!wget -O /content/en_ewt-ud-dev.conllu   https://raw.githubusercontent.com/UniversalDependencies/UD_English-EWT/master/en_ewt-ud-dev.conllu\n",
        "!wget -O /content/en_ewt-ud-test.conllu  https://raw.githubusercontent.com/UniversalDependencies/UD_English-EWT/master/en_ewt-ud-test.conllu\n",
        "\n"
      ],
      "metadata": {
        "id": "OSS4MWGkZw2m",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "collapsed": true,
        "outputId": "a08eb393-ddfd-4f3e-ff63-660287cc5666"
      },
      "execution_count": 201,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "--2025-11-22 13:14:26--  https://raw.githubusercontent.com/UniversalDependencies/UD_English-EWT/master/en_ewt-ud-train.conllu\n",
            "Resolving raw.githubusercontent.com (raw.githubusercontent.com)... 185.199.108.133, 185.199.109.133, 185.199.110.133, ...\n",
            "Connecting to raw.githubusercontent.com (raw.githubusercontent.com)|185.199.108.133|:443... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 15029817 (14M) [text/plain]\n",
            "Saving to: ‘/content/en_ewt-ud-train.conllu’\n",
            "\n",
            "\r          /content/   0%[                    ]       0  --.-KB/s               \r/content/en_ewt-ud- 100%[===================>]  14.33M  --.-KB/s    in 0.1s    \n",
            "\n",
            "2025-11-22 13:14:26 (133 MB/s) - ‘/content/en_ewt-ud-train.conllu’ saved [15029817/15029817]\n",
            "\n",
            "--2025-11-22 13:14:26--  https://raw.githubusercontent.com/UniversalDependencies/UD_English-EWT/master/en_ewt-ud-dev.conllu\n",
            "Resolving raw.githubusercontent.com (raw.githubusercontent.com)... 185.199.108.133, 185.199.109.133, 185.199.110.133, ...\n",
            "Connecting to raw.githubusercontent.com (raw.githubusercontent.com)|185.199.108.133|:443... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 1898013 (1.8M) [text/plain]\n",
            "Saving to: ‘/content/en_ewt-ud-dev.conllu’\n",
            "\n",
            "/content/en_ewt-ud- 100%[===================>]   1.81M  --.-KB/s    in 0.06s   \n",
            "\n",
            "2025-11-22 13:14:26 (32.6 MB/s) - ‘/content/en_ewt-ud-dev.conllu’ saved [1898013/1898013]\n",
            "\n",
            "--2025-11-22 13:14:26--  https://raw.githubusercontent.com/UniversalDependencies/UD_English-EWT/master/en_ewt-ud-test.conllu\n",
            "Resolving raw.githubusercontent.com (raw.githubusercontent.com)... 185.199.108.133, 185.199.109.133, 185.199.110.133, ...\n",
            "Connecting to raw.githubusercontent.com (raw.githubusercontent.com)|185.199.108.133|:443... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 1899759 (1.8M) [text/plain]\n",
            "Saving to: ‘/content/en_ewt-ud-test.conllu’\n",
            "\n",
            "/content/en_ewt-ud- 100%[===================>]   1.81M  --.-KB/s    in 0.06s   \n",
            "\n",
            "2025-11-22 13:14:26 (30.6 MB/s) - ‘/content/en_ewt-ud-test.conllu’ saved [1899759/1899759]\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "data_train_childes = open(\"/content/en_childes-ud-train.conllu\", \"r\", encoding=\"utf-8\").read()\n",
        "data_dev_childes = open(\"/content/en_childes-ud-dev.conllu\", \"r\", encoding=\"utf-8\").read()\n",
        "data_test_childes = open(\"/content/en_childes-ud-test.conllu\", \"r\", encoding=\"utf-8\").read()\n",
        "\n",
        "print(\"There are {} sentences in the training set.\".format(len(data_train_childes)))\n",
        "print(\"There are {} sentences in the testing set.\".format(len(data_test_childes)))\n",
        "print(\"There are {} sentences in the development set.\".format(len(data_dev_childes)))"
      ],
      "metadata": {
        "id": "7uV0tsLeaIgH",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "5f43f713-3c89-4297-cfcd-71bd9e287ca2"
      },
      "execution_count": 202,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "There are 19558483 sentences in the training set.\n",
            "There are 5194505 sentences in the testing set.\n",
            "There are 2159081 sentences in the development set.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "data_train_ewt = open(\"/content/en_ewt-ud-train.conllu\", \"r\", encoding=\"utf-8\").read()\n",
        "data_dev_ewt = open(\"/content/en_ewt-ud-dev.conllu\", \"r\", encoding=\"utf-8\").read()\n",
        "data_test_ewt = open(\"/content/en_ewt-ud-test.conllu\", \"r\", encoding=\"utf-8\").read()\n",
        "\n",
        "print(\"There are {} sentences in the training set.\".format(len(data_train_ewt)))\n",
        "print(\"There are {} sentences in the testing set.\".format(len(data_test_ewt)))\n",
        "print(\"There are {} sentences in the development set.\".format(len(data_dev_ewt)))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "6OYRm3rrbBqG",
        "outputId": "f899feaa-e153-40f7-bb7e-1d86bd97b185",
        "collapsed": true
      },
      "execution_count": 203,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "There are 15028897 sentences in the training set.\n",
            "There are 1899745 sentences in the testing set.\n",
            "There are 1897963 sentences in the development set.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "print(data_train_childes[:500])\n",
        "print(\"----------------------------------\")\n",
        "#print(data_train_ewt[:500])"
      ],
      "metadata": {
        "id": "7pbdZpQgaNWl",
        "collapsed": true,
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "2fd32a48-4e7a-41fd-e9cf-d6ce4df72c13"
      },
      "execution_count": 204,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "# sent_id = 0\n",
            "# original_sent_id = 1754162\n",
            "# s_24_sent_id = 14944\n",
            "# child_name = Adam\n",
            "# childes_toks = you won't cut your finger what\n",
            "# corpus_name = Brown\n",
            "# gold_annotation = False\n",
            "# child_age = 41.95363354483665\n",
            "# child_gender = male\n",
            "# speaker_role = Investigator\n",
            "# type = question\n",
            "# text = You won 't cut your finger what?\n",
            "1\tYou\tyou\tPRON\tPRP\t_\t4\tnsubj\t4:nsubj\t_\n",
            "2\twon\twill\tAUX\tVBD\t_\t4\taux\t4:aux\t_\n",
            "3\t't\tnot\tPART\tRB\t_\t4\tadvmod\t4:advmod\t_\n",
            "4\tcut\tcut\tVERB\tVB\t_\t0\troot\t0:root\t_\n",
            "5\tyour\tyour\tPRON\tPRP$\t_\t6\n",
            "----------------------------------\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "%cd /content/hmm-pos-tagger"
      ],
      "metadata": {
        "id": "FX_uSJDRi4KL",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "collapsed": true,
        "outputId": "26a09792-d7f7-4abf-9632-dadad59cf7ef"
      },
      "execution_count": 205,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "/content/hmm-pos-tagger\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# CHILDES dataset\n",
        "with open(\"en_childes-ud-train.conllu\", \"w\", encoding=\"utf-8\") as f:\n",
        "    f.write(data_train_childes)\n",
        "\n",
        "with open(\"en_childes-ud-dev.conllu\", \"w\", encoding=\"utf-8\") as f:\n",
        "    f.write(data_dev_childes)\n",
        "\n",
        "with open(\"en_childes-ud-test.conllu\", \"w\", encoding=\"utf-8\") as f:\n",
        "    f.write(data_test_childes)\n",
        "\n",
        "# EWT dataset\n",
        "with open(\"en_ewt-ud-train.conllu\", \"w\", encoding=\"utf-8\") as f:\n",
        "    f.write(data_train_ewt)\n",
        "\n",
        "with open(\"en_ewt-ud-dev.conllu\", \"w\", encoding=\"utf-8\") as f:\n",
        "    f.write(data_dev_ewt)\n",
        "\n",
        "with open(\"en_ewt-ud-test.conllu\", \"w\", encoding=\"utf-8\") as f:\n",
        "    f.write(data_test_ewt)\n"
      ],
      "metadata": {
        "id": "U22KFy7rjdW7"
      },
      "execution_count": 206,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#!git clone https://github.com/mohammel-debug/hmm-pos-tagger.git\n",
        "#%cd hmm-pos-tagger\n",
        "#!mv en_childes-ud-train.conllu hmm-pos-tagger/\n",
        "#!mv en_childes-ud-dev.conllu   hmm-pos-tagger/\n",
        "#!mv en_childes-ud-test.conllu  hmm-pos-tagger/"
      ],
      "metadata": {
        "collapsed": true,
        "id": "OTwmaj2JO-Wg"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import nltk\n",
        "!pip install conllu\n",
        "nltk.download('universal_tagset')\n"
      ],
      "metadata": {
        "id": "4zweq72x6zRl",
        "collapsed": true
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def parse_data(file1):\n",
        "\n",
        "  sentences = []\n",
        "  pos_tags = []\n",
        "\n",
        "  current_sent_tokens = []\n",
        "  current_sent_tags = []\n",
        "\n",
        "  lines = file1.strip().split('\\n')\n",
        "\n",
        "  for line in lines:\n",
        "      line = line.strip()\n",
        "\n",
        "      # Skip comments/metadata and empty lines\n",
        "      if line.startswith(\"#\") or not line:\n",
        "          # If we have accumulated tokens for a sentence, save them now\n",
        "          if current_sent_tokens:\n",
        "              sentences.append(current_sent_tokens)\n",
        "              pos_tags.append(current_sent_tags)\n",
        "              current_sent_tokens = []\n",
        "              current_sent_tags = []\n",
        "          continue\n",
        "\n",
        "      parts = line.split('\\t')\n",
        "\n",
        "      # Safety check: ensure line has enough columns\n",
        "      if len(parts) < 4:\n",
        "          continue\n",
        "\n",
        "      # Skip multi-word ranges (e.g., \"1-2\") because the individual tokens follow\n",
        "      if '-' in parts[0]:\n",
        "          continue\n",
        "\n",
        "      # Append Token (Column 1) and UPOS (Column 3)\n",
        "      # Note: If you want the specific XPOS tag (e.g., PRP, VBD), change index 3 to 4\n",
        "      current_sent_tokens.append(parts[1])\n",
        "      current_sent_tags.append(parts[3])\n",
        "\n",
        "  # Flush the final sentence if the file didn't end with a newline\n",
        "  if current_sent_tokens:\n",
        "      sentences.append(current_sent_tokens)\n",
        "      pos_tags.append(current_sent_tags)\n",
        "\n",
        "  # --- Output Results ---\n",
        "  print(f\"Total Sentences found: {len(sentences)}\\n\")\n",
        "  return sentences, pos_tags\n"
      ],
      "metadata": {
        "id": "iXsteXI_B7NJ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "sentences_train, pos_tags_train = parse_data(data_train)\n",
        "sentences_dev, pos_tags_dev = parse_data(data_dev)\n",
        "sentences_test, pos_tags_test = parse_data(data_test)\n"
      ],
      "metadata": {
        "id": "zJIjWQ_CDAy6",
        "collapsed": true
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "tags = set()\n",
        "for word_l, tag_l in zip(sentences_train, pos_tags_train):\n",
        "  for tag in tag_l:\n",
        "    tags.add(tag)\n",
        "# retrieve the total number of tags in the tagset and the set of tags\n",
        "print(f\"Total number of tags: {len(tags)}\")\n",
        "print(tags)\n"
      ],
      "metadata": {
        "id": "i4l0GIXBCff4"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "tags = list()\n",
        "for word, tag_l in zip(sentences_train, pos_tags_train):\n",
        "  for tag in tag_l:\n",
        "    tags.append(tag)\n",
        "freq = nltk.FreqDist(tags)\n",
        "freq.plot()"
      ],
      "metadata": {
        "id": "E6GnHi4oECcR",
        "collapsed": true
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "tags = list()\n",
        "for word, tag_l in zip(sentences_dev, pos_tags_dev):\n",
        "  for tag in tag_l:\n",
        "    tags.append(tag)\n",
        "freq = nltk.FreqDist(tags)\n",
        "freq.plot()"
      ],
      "metadata": {
        "id": "lohM24EwId0q",
        "collapsed": true
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "tags = list()\n",
        "for word, tag_l in zip(sentences_test, pos_tags_test):\n",
        "  for tag in tag_l:\n",
        "    tags.append(tag)\n",
        "freq = nltk.FreqDist(tags)\n",
        "freq.plot()"
      ],
      "metadata": {
        "id": "3dShzlbwI41D",
        "collapsed": true
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#train_set = list()\n",
        "# for word_l, tag_l in zip(sentences_test, pos_tags_test):\n",
        "#   for word, tag in zip(word_l, tag_l):\n",
        "# #     train_set.append((word,tag))\n",
        "# train_set = list(zip(zip(sentences_train, pos_tags_train)))\n",
        "# print(train)\n",
        "tagged_sentences_train = [list(zip(s, p)) for s, p in zip(sentences_train, pos_tags_train)]\n",
        "tagged_sentences_test = [list(zip(s, p)) for s, p in zip(sentences_test, pos_tags_test)]\n",
        "tagged_sentences_dev = [list(zip(s, p)) for s, p in zip(sentences_dev, pos_tags_dev)]"
      ],
      "metadata": {
        "id": "8MkO_xnoI_8_"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "tagger = nltk.tag.hmm.HiddenMarkovModelTagger.train(tagged_sentences_train)"
      ],
      "metadata": {
        "id": "pP9K_TlUK-Wh"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "tagger.accuracy(tagged_sentences_test)"
      ],
      "metadata": {
        "id": "aS7Kz0FOLOff"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "90_NIimRMUBx"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "TODO\n",
        "\n",
        "- Same method. Try with the other dataset\n",
        "- Combine datasets and train.\n",
        "- Test with one and the other dataset. Combined.\n",
        "- Plot the accuracies. Analyse.\n",
        "- How to improve.\n",
        "- Analyse the error per tag, and the reason. For each model.\n",
        "- Check nltk for interesting methods.\n",
        "- See if we can add more things from the unfinished chunking lab."
      ],
      "metadata": {
        "id": "7i-i4zyGNLdz"
      }
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "collapsed": true,
        "id": "-XKZu9KmBgsa"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}